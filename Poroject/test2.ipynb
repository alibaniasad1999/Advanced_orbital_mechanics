{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 14:52:23.330129: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 276ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "step() missing 1 required positional argument: 'u'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m     agent\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mmodel.h5\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 78\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[2], line 74\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m env \u001b[39m=\u001b[39m MassDamperSpring(m\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, d\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, k\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, dt\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m     73\u001b[0m agent \u001b[39m=\u001b[39m Agent(env, lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.95\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m simulate(agent, env, episodes\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, max_steps\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, render\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     75\u001b[0m agent\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mmodel.h5\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 62\u001b[0m, in \u001b[0;36msimulate\u001b[0;34m(agent, env, episodes, max_steps, render)\u001b[0m\n\u001b[1;32m     60\u001b[0m     env\u001b[39m.\u001b[39mrender()\n\u001b[1;32m     61\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(state)\n\u001b[0;32m---> 62\u001b[0m next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     63\u001b[0m next_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(next_state, [\u001b[39m1\u001b[39m, agent\u001b[39m.\u001b[39mstate_dim])\n\u001b[1;32m     64\u001b[0m agent\u001b[39m.\u001b[39mtrain(state, action, reward, next_state, done)\n",
      "\u001b[0;31mTypeError\u001b[0m: step() missing 1 required positional argument: 'u'"
     ]
    }
   ],
   "source": [
    "# Define the dynamics of the mass-damper-spring system for reinforcement learning\n",
    "class MassDamperSpring:\n",
    "    def __init__(self, m, d, k, dt):\n",
    "        self.m = m\n",
    "        self.d = d\n",
    "        self.k = k\n",
    "        self.dt = dt\n",
    "\n",
    "    def step(self, x, u):\n",
    "        x1, x2 = x\n",
    "        dx1 = x2\n",
    "        dx2 = (-self.d * x2 - self.k * x1 + u) / self.m\n",
    "        x1 += dx1 * self.dt\n",
    "        x2 += dx2 * self.dt\n",
    "        return np.array([x1, x2])\n",
    "\n",
    "    def reset(self):\n",
    "        return np.array([0.0, 0.0])\n",
    "\n",
    "# Define the reinforcement learning agent\n",
    "class Agent:\n",
    "    def __init__(self, env, lr, gamma):\n",
    "        self.env = env\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.state_dim = env.reset().shape[0]\n",
    "        self.action_dim = 1\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(16, input_dim=self.state_dim, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_dim, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=self.lr))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        return self.model.predict(state)\n",
    "\n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "        target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "        target_f = self.model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "# Define the simulation\n",
    "def simulate(agent, env, episodes, max_steps, render=False):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, agent.state_dim])\n",
    "        for step in range(max_steps):\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, agent.state_dim])\n",
    "            agent.train(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(episode+1, episodes, step))\n",
    "                break\n",
    "\n",
    "# Define the main function\n",
    "def main():\n",
    "    env = MassDamperSpring(m=1.0, d=0.1, k=1.0, dt=0.01)\n",
    "    agent = Agent(env, lr=0.001, gamma=0.95)\n",
    "    simulate(agent, env, episodes=100, max_steps=1000, render=False)\n",
    "    agent.save(\"model.h5\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
