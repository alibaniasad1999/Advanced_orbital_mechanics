{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the equations of motion for the CRTBP environment \n",
    "class CRTBPEnv(gym.Env):\n",
    "    def __init__(self, initial_condition, finial_state, mu):\n",
    "        self.initial_condition = initial_condition\n",
    "        self.finial_state = finial_state\n",
    "        self.state = None\n",
    "        if mu == None:\n",
    "            self.mu = 0.012277471\n",
    "        else:\n",
    "            self.mu = mu\n",
    "        # Define the action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Box(low=-.1, high=.1, shape=(2,), dtype=np.float32)\n",
    "        # Example for using image as input:\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        mu = self.mu\n",
    "        # CRTBP equations in X-Y plane\n",
    "        # X = x, y, vx, vy\n",
    "        # U = mu\n",
    "        # dot_X = f(X, U) = AX + BU\n",
    "        # y = Cx + DU\n",
    "        X = self.state\n",
    "        A = np.array([[0, 0, 1, 0],\n",
    "                        [0, 0, 0, 1],\n",
    "                        [1-(1-mu)/((X[0]+mu)**2+X[1]**2)**(3/2)-mu/((X[0]-1+mu)**2+X[1]**2)**(3/2), 0, 0 , 2],\n",
    "                        [0, 1-(1-mu)/((X[0]+mu)**2+X[1]**2)**(3/2)-mu/((X[0]-1+mu)**2+X[1]**2)**(3/2), -2, 0]])\n",
    "        B = np.array([[0],\n",
    "                     [0],\n",
    "                     [mu*(1-mu)/((X[0]+mu)**2+X[1]**2)**(3/2)+mu*(1-mu)/((X[0]-1+mu)**2+X[1]**2)**(3/2)],\n",
    "                        [0]])\n",
    "        C = np.array([[1, 0, 0, 0],\n",
    "                        [0, 1, 0, 0]])\n",
    "        D = np.array([0, 0])\n",
    "        dot_X = np.dot(A, X) + B + np.array([0, 0, action[0], action[1]])\n",
    "\n",
    "        observation = X + dot_X\n",
    "\n",
    "        # Calculate the reward\n",
    "        reward = -np.linalg.norm(observation - self.finial_state)\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = bool(np.linalg.norm(observation - self.finial_state) < 0.001)\n",
    "\n",
    "        info = {}\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        self.state = self.initial_condition\n",
    "        return self.initial_condition\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        # Render the environment to the screen\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.94000000e-01 -2.00158511e+00 -4.93373545e+04 -1.99813006e+00]\n",
      " [ 9.94000000e-01 -2.00158511e+00 -4.93373545e+04 -1.99813006e+00]\n",
      " [ 4.90228598e+04  4.90198642e+04 -3.15488676e+02  4.90198677e+04]\n",
      " [ 9.94000000e-01 -2.00158511e+00 -4.93373545e+04 -1.99813006e+00]]\n",
      "-120465.1703843846\n",
      "False\n",
      "{}\n",
      "[0.03054477 0.00345505]\n"
     ]
    }
   ],
   "source": [
    "# create earth moon three body problem #\n",
    "# initial condition #\n",
    "initial_condition = np.array([0.994, 0, 0, -2.00158510637908252240537862224])\n",
    "# finial state #\n",
    "finial_state = np.array([0.994, 0, 0, -2.00158510637908252240537862224])\n",
    "# create the environment #\n",
    "env = CRTBPEnv(initial_condition, finial_state, mu = 0.012277471)\n",
    "# reset the environment #\n",
    "env.reset()\n",
    "# take a random action #\n",
    "action = env.action_space.sample()\n",
    "# take a step #\n",
    "observation, reward, done, info = env.step(action)\n",
    "# print the observation #\n",
    "print(observation)\n",
    "# print the reward #\n",
    "print(reward)\n",
    "# print the done #\n",
    "print(done)\n",
    "# print the info #\n",
    "print(info)\n",
    "# print the action #\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 3 body problem transfer reinforcment learning agent\n",
    "class ThreeBodyProblemAgent():\n",
    "    def __init__(self, env):\n",
    "        self.is_discrete = \\\n",
    "            type(env.action_space) == gym.spaces.discrete.Discrete\n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n\n",
    "            print(\"Action size:\", self.action_size)\n",
    "        \n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "            print(\"Action range:\", self.action_low, self.action_high)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        if self.is_discrete:\n",
    "            action = np.random.choice(range(self.action_size))\n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low, self.action_high, self.action_shape)\n",
    "        return action\n",
    "    \n",
    "# create the agent #\n",
    "agent = ThreeBodyProblemAgent(env)\n",
    "# get the action #\n",
    "action = agent.get_action(env.reset())\n",
    "# print the action #\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent learning #\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        # Initialize parameters\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.action_low = env.action_space.low\n",
    "        self.action_high = env.action_space.high\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.memory = deque(maxlen=100000)\n",
    "\n",
    "        # Initialize discount and exploration rate\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        # Initialize model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Build a neural network\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(48, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=0.001))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # Store experiences in memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        # Choose an action based on given state\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # Train the neural network with experiences in memory\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if not done:\n",
    "                target = reward + \\\n",
    "                         self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target = reward\n",
    "            train_target = self.model.predict(state)\n",
    "            train_target[0][action] = target\n",
    "            self.model.fit(state, train_target, verbose=0)\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "\n",
    "    def load(self, name):\n",
    "        # Load the neural network weights\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        # Save the neural network weights\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "# create the agent #\n",
    "agent = DQNAgent(env)\n",
    "# print the state size #\n",
    "print(agent.state_size)\n",
    "# print the action size #\n",
    "print(agent.action_size)\n",
    "# print the action low #\n",
    "print(agent.action_low)\n",
    "# print the action high #\n",
    "print(agent.action_high)\n",
    "# print the gamma #\n",
    "print(agent.gamma)\n",
    "# print the epsilon #\n",
    "print(agent.epsilon)\n",
    "# print the epsilon decay #\n",
    "print(agent.epsilon_decay)\n",
    "# print the epsilon min #\n",
    "print(agent.epsilon_min)\n",
    "# print the model #\n",
    "print(agent.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# classical gym \n",
    "import gym\n",
    "# instead of gym, import gymnasium \n",
    "#import gymnasium as gym\n",
    "\n",
    "# create environment\n",
    "# env=gym.make('CartPole-v1')\n",
    "\n",
    "# select the parameters\n",
    "gamma=1\n",
    "# probability parameter for the epsilon-greedy approach\n",
    "epsilon=0.1\n",
    "# number of training episodes\n",
    "# NOTE HERE THAT AFTER CERTAIN NUMBERS OF EPISODES, WHEN THE PARAMTERS ARE LEARNED\n",
    "# THE EPISODE WILL BE LONG, AT THAT POINT YOU CAN STOP THE TRAINING PROCESS BY PRESSING CTRL+C\n",
    "# DO NOT WORRY, THE PARAMETERS WILL BE MEMORIZED\n",
    "numberEpisodes=1000\n",
    "\n",
    "# create an object\n",
    "LearningQDeep=DeepQLearning(env,0.99,1,10)\n",
    "\n",
    "# run the learning process\n",
    "LearningQDeep.trainingEpisodes()\n",
    "# get the obtained rewards in every episode\n",
    "LearningQDeep.sumRewardsEpisode\n",
    "\n",
    "#  summarize the model\n",
    "LearningQDeep.mainNetwork.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Define the RL agent\n",
    "class RLAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.q_table = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return np.argmax(self.q_table[state, :])\n",
    "\n",
    "    def train(self, num_episodes, learning_rate, discount_rate, epsilon):\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                # Exploration vs. Exploitation\n",
    "                if np.random.rand() <= epsilon:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = self.get_action(state)\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                # Q-Learning update\n",
    "                q_value = self.q_table[state, action]\n",
    "                max_q_value = np.max(self.q_table[next_state, :])\n",
    "                new_q_value = q_value + learning_rate * (reward + discount_rate * max_q_value - q_value)\n",
    "                self.q_table[state, action] = new_q_value\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "# Create the environment\n",
    "# env = gym.make('CartPole-v1')\n",
    "\n",
    "# Create the RL agent\n",
    "agent = RLAgent(env)\n",
    "\n",
    "# Set hyperparameters\n",
    "num_episodes = 1000\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "epsilon = 0.1\n",
    "\n",
    "# Train the agent\n",
    "agent.train(num_episodes, learning_rate, discount_rate, epsilon)\n",
    "\n",
    "# Test the agent\n",
    "total_rewards = 0\n",
    "num_test_episodes = 10\n",
    "for _ in range(num_test_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_rewards += reward\n",
    "\n",
    "average_reward = total_rewards / num_test_episodes\n",
    "print(\"Average reward:\", average_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 21:38:48.641617: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 134ms/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     43\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mget_action(state)\n\u001b[0;32m---> 44\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     45\u001b[0m     agent\u001b[39m.\u001b[39mtrain(state, action, reward, next_state, done)\n\u001b[1;32m     46\u001b[0m     state \u001b[39m=\u001b[39m next_state\n",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m, in \u001b[0;36mCRTBPEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m C \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m],\n\u001b[1;32m     35\u001b[0m                 [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]])\n\u001b[1;32m     36\u001b[0m D \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m])\n\u001b[0;32m---> 37\u001b[0m dot_X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(A, X) \u001b[39m+\u001b[39m B \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, action[\u001b[39m0\u001b[39;49m], action[\u001b[39m1\u001b[39m]])\n\u001b[1;32m     39\u001b[0m observation \u001b[39m=\u001b[39m X \u001b[39m+\u001b[39m dot_X\n\u001b[1;32m     41\u001b[0m \u001b[39m# Calculate the reward\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the RL agent using TensorFlow\n",
    "class RLAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(action_size, activation='softmax')\n",
    "        ])\n",
    "        # Define any additional components or parameters for your RL agent\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        action_probs = self.model.predict(state)[0]\n",
    "        action = np.random.choice(range(self.action_size), p=action_probs)\n",
    "        return action\n",
    "\n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        next_state = np.reshape(next_state, [1, self.state_size])\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target += np.amax(self.model.predict(next_state)[0])\n",
    "        target_f = self.model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "# Create the three-body problem transfer environment and RL agent\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "agent = RLAgent(state_size, action_size)\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.train(state, action, reward, next_state, done)\n",
    "        state = next_state\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
